{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gu6T81g6hM_"
      },
      "source": [
        "**Importazione delle librerie utili allo sviluppo dei modelli**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zh9U4OEthVru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNkSAgIupQ9E"
      },
      "outputs": [],
      "source": [
        "import os as os\n",
        "from itertools import product\n",
        "from itertools import islice\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "from datetime import datetime\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "import math\n",
        "import scipy.stats as sts\n",
        "from itertools import chain\n",
        "\n",
        "# Anonymize DF\n",
        "from anonymizedf.anonymizedf import anonymize\n",
        "import dataframe_image as dfi\n",
        "\n",
        "from pandas import concat\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import xgboost as xgb\n",
        "\n",
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.dates as mdates\n",
        "from sklearn.metrics import mean_squared_log_error\n",
        "from datetime import timedelta\n",
        "from pandas.tseries.offsets import MonthEnd\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#import custom function to explore dataset values and plot\n",
        "#from src.explore_functions import df_value_stat\n",
        "\n",
        "#Inizialize constants\n",
        "RANDOM_SEED = 42\n",
        "SAMPLE_NUMBER = 5\n",
        "N_COMB = 10\n",
        "\n",
        "#Dataset source commentare uno o l'altro dipende dalla posizione del dataset\n",
        "#DATASET_SOURCE = \"dataset.xlsx\"\n",
        "#SHEET_NAME = \"Sheet1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0vdG0U3YsDj"
      },
      "source": [
        "**Settaggio di alcune opzioni grafiche**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Am6iaPpcYmHL"
      },
      "outputs": [],
      "source": [
        "sns.set()\n",
        "sns.set_style(\"darkgrid\")\n",
        "sns.set_palette(\"inferno_r\")\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = \"retina\"\n",
        "plt.rcParams['figure.figsize'] = (15.0, 10.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRCEulGZ6ikp"
      },
      "source": [
        "**Rinomino i clienti in modo più comprensibile**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "9rgl7xE_6ikp"
      },
      "outputs": [],
      "source": [
        "def rinomina_clienti(df_input):\n",
        "    anon = anonymize(df_input)\n",
        "    anon.fake_categories(\"Cliente\")\n",
        "    remap = {}\n",
        "    fake_cliente = df_input['Fake_Cliente'].unique().tolist()\n",
        "    cliente = df_input['Cliente'].unique().tolist()\n",
        "    for key in fake_cliente:\n",
        "        for value in cliente:\n",
        "            remap[key] = value\n",
        "            cliente.remove(value)\n",
        "            break\n",
        "    display(remap)\n",
        "    df_input = df_input.assign(Cliente = df['Fake_Cliente'])\n",
        "    df_input = df_input.drop(columns='Fake_Cliente')\n",
        "    return df_input, remap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWHB1VXG6ikq"
      },
      "source": [
        "**Funzione che permette di tornare ad ottenere i clienti col proprio ID/nome**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8woqwOF6ikq"
      },
      "outputs": [],
      "source": [
        "def rinomina_clienti_back(df_input, remap):\n",
        "    df_input['Cliente'] = df_input['Cliente'].map(remap)\n",
        "    return df_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRj1TFkb6ikq"
      },
      "outputs": [],
      "source": [
        "df, remap = rinomina_clienti(df_input=df)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N97-Fh8T6ikq"
      },
      "source": [
        "# Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxK2Ebog6ikq"
      },
      "source": [
        "**Analisi delle componenti del dataset per variabile d'interesse**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gBHUNY36ikr"
      },
      "outputs": [],
      "source": [
        "def makeDataLagged(data, n_in=1, n_out=1, dropnan=True):\n",
        "    \"\"\"\n",
        "    Lagga tutte le colonne nei dati\n",
        "    Argomenti:\n",
        "    data: sequenza di osservazioni, lista o Numpy array.\n",
        "    n_in: Numero di osservazioni laggate come input (X).\n",
        "    n_out: Numero di osservazioni come output (y).\n",
        "    dropnan: Booleana, sceglie se eliminare o no le righe con valori NaN.\n",
        "    Ritorna:\n",
        "    Pandas dataframe o np array\n",
        "    \"\"\"\n",
        "    n_vars = 1 if type(data) is list else data.shape[1]\n",
        "    df = pd.DataFrame(data)\n",
        "    cols, names = list(), list()\n",
        "        # sequenza input (t-n, ... t-1)\n",
        "    for i in range(n_in, 0, -1):\n",
        "        cols.append(df.shift(i))\n",
        "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "        # sequenza di forecast (t, t+1, ... t+n)\n",
        "    for i in range(0, n_out):\n",
        "        cols.append(df.shift(-i))\n",
        "        if i == 0:\n",
        "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "        else:\n",
        "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "        # mette tutto insieme\n",
        "        agg = concat(cols, axis=1)\n",
        "        agg.columns = names\n",
        "        # droppa le righe coi valori NaN\n",
        "    if dropnan:\n",
        "        agg.dropna(inplace=True)\n",
        "    \n",
        "    return agg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9QWe_Yt6ikr"
      },
      "source": [
        "### Automatizzazione del processo di allenamento dei modelli e sviluppo dei forecast con funzioni e loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxKxa-L46ikr"
      },
      "source": [
        "Scelgo il range di tempo su cui fare previsioni, i modelli e gli iperparametri su cui fare il tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2AY3igE6ikr"
      },
      "outputs": [],
      "source": [
        "# Creazione del nuovo indice con intervallo mensile compreso tra '2020-01-01' e '2022-12-01'\n",
        "#AGGIORNARE NEL CASO SI VOGLIA ANDARE AVANTI CON LE PREVISIONI\n",
        "new_index = pd.date_range(start='2020-01-01', end='2022-12-01', freq='MS')\n",
        "\n",
        "# define the models\n",
        "models = {'Random Forest': RandomForestRegressor(),\n",
        "          'XG Boost': xgb.XGBRegressor(verbosity=0 ,random_state=RANDOM_SEED),\n",
        "          'Ada Boost': AdaBoostRegressor(random_state=RANDOM_SEED),\n",
        "          'K-nn': KNeighborsRegressor(n_jobs=-1),\n",
        "          'Decision Tree': DecisionTreeRegressor(random_state=RANDOM_SEED)}\n",
        "\n",
        "# Hyperparameter tuning using RandomizedSearchCV\n",
        "param_grid = {'Random Forest': {'n_estimators': [10, 20, 50, 100],\n",
        "                                'max_features': ['sqrt', 'log2'],\n",
        "                                'max_depth' : [i for i in range(5,25)]\n",
        "                                },\n",
        "              'XG Boost': {'n_estimators': [10, 20, 50, 100],\n",
        "                           'max_features': ['sqrt', 'log2'],\n",
        "                           'max_depth' : [i for i in range(5,25)]\n",
        "                          },\n",
        "              'Ada Boost' : {'n_estimators': [10, 20, 50, 100],\n",
        "                             'loss': ['linear', 'square','exponential'],\n",
        "                             'learning_rate' : [i for i in [0.1,0.3,0.5,0.7,0.9,1]]\n",
        "                            },\n",
        "              'K-nn': {'n_neighbors': [1,2,3],\n",
        "                       'weights': ['uniform', 'distance']\n",
        "                      },\n",
        "              'Decision Tree': {'criterion': ['squared_error', 'absolute_error','friedman_mse'],\n",
        "                                'max_features': [\"auto\", \"sqrt\", \"log2\"],\n",
        "                                'max_depth' : [i for i in range(5,25)]\n",
        "                                }\n",
        "             }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJ6jouJf6iks"
      },
      "source": [
        "**Creazione delle funzioni per ottenere i dataframe con le varie combinazioni di feature, e creare una lista di dataframe con questi**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRpnwjnm6iks"
      },
      "outputs": [],
      "source": [
        "def makeBestComb(df, features, N_COMB = 10):\n",
        "    '''\n",
        "    Calcolo il numero di vendite e occorenze per ogni combinazione possibile dato un insieme di features\n",
        "     \n",
        "    df : dataframe su cui trova combinazioni e calcola la somma e l occorenza per ogni combinazione\n",
        "    features : insieme di feature, aggrega per ogni combinazione di queste partendo da 1 a n_feature\n",
        "    N_COMB : numero di combinazioni migliori da prendere per ogni combinazione di features\n",
        "   \n",
        "    Restituisce una lista di dataframe\n",
        "    '''\n",
        "    i = 0\n",
        "    var = list()\n",
        "    combo = list()\n",
        "    somma_tot = df['Vendite_consuntive'].sum()\n",
        "    count_tot = df['Vendite_consuntive'].count()\n",
        "    \n",
        "    for feature in features:\n",
        "        var.append(feature)\n",
        "        combo.append(df.groupby(var)['Vendite_consuntive'].agg(['sum', 'count']).sort_values(by='sum', ascending=False).reset_index())\n",
        "        \n",
        "        display(combo[i].head(N_COMB))\n",
        "        print(\"Somma Vendite Consuntive Considerate\")\n",
        "        print(combo[i]['sum'].head(N_COMB).sum())\n",
        "        print(\"Somma Occorrenze Considerate\")\n",
        "        print(combo[i]['count'].head(N_COMB).sum())\n",
        "        print(\"Rapporto Vendite Consuntive Totali/Considerate\")\n",
        "        print(combo[i]['sum'].head(N_COMB).sum()/somma_tot)\n",
        "        print(\"Rapporto Occorrenze Totali/Considerate\")\n",
        "        print(combo[i]['count'].head(N_COMB).sum()/count_tot)\n",
        "        \n",
        "        i += 1\n",
        "    return combo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YK2xrt16iks"
      },
      "outputs": [],
      "source": [
        "def makeDataframeFromCombo(df, combo, features, N_COMB = 10):\n",
        "    '''\n",
        "    Per ogni top combinazione trovata prima, crea un dataframe contenente tutte le transazioni con quella combinazione\n",
        "    \n",
        "    df : Dataframe da cui prende le transazioni\n",
        "    combo : dataframe contente le combinazioni top (+ vendite tot/+ occorrenze)\n",
        "    features : insieme di feature, aggrega per ogni combinazione di queste partendo da 1 a n_feature\n",
        "    N_COMB : numero di combinazioni migliori da prendere per ogni combinazione di features\n",
        "    \n",
        "    Restituisce una matrice di dataframe con tutte le combinazioni\n",
        "    Il primo indice indica quante feature sono prese in considerazioni per la combinazione\n",
        "    Il secondo indice indica quale tra le top N_COMB prendere\n",
        "    --> df_comb[4][0] --> Prende il dataframe della combinazione con più vendite [0], \n",
        "    tra quelli con la combinazioni di 5 features [4]\n",
        "    '''\n",
        "    df_comb = list()\n",
        "    df_comb.append(list())\n",
        "    x = 0\n",
        "    for i in range(N_COMB):\n",
        "        df_comb[x].append(df[df[features[x]] == combo[x].iloc[i][x]])\n",
        "    x += 1\n",
        "    df_comb.append(list())\n",
        "    for i in range(N_COMB):\n",
        "        df_comb[x].append(df[(df[features[x-1]] == combo[x].iloc[i][x-1]) &\n",
        "                     (df[features[x]] == combo[x].iloc[i][x])])\n",
        "    x += 1\n",
        "    df_comb.append(list())\n",
        "    for i in range(N_COMB):\n",
        "        df_comb[x].append(df[(df[features[x-2]] == combo[x].iloc[i][x-2]) &\n",
        "                     (df[features[x-1]] == combo[x].iloc[i][x-1]) &\n",
        "                     (df[features[x]] == combo[x].iloc[i][x])])\n",
        "    x += 1\n",
        "    df_comb.append(list())\n",
        "    for i in range(N_COMB):\n",
        "        df_comb[x].append(df[(df[features[x-3]] == combo[x].iloc[i][x-3]) &\n",
        "                     (df[features[x-2]] == combo[x].iloc[i][x-2]) &\n",
        "                     (df[features[x-1]] == combo[x].iloc[i][x-1]) &\n",
        "                     (df[features[x]] == combo[x].iloc[i][x])])\n",
        "    x += 1\n",
        "    df_comb.append(list())\n",
        "    for i in range(N_COMB):\n",
        "        df_comb[x].append(df[(df[features[x-4]] == combo[x].iloc[i][x-4]) &\n",
        "                     (df[features[x-3]] == combo[x].iloc[i][x-3]) &\n",
        "                     (df[features[x-2]] == combo[x].iloc[i][x-2]) &\n",
        "                     (df[features[x-1]] == combo[x].iloc[i][x-1]) &\n",
        "                     (df[features[x]] == combo[x].iloc[i][x])])\n",
        "        \n",
        "    return df_comb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA8N8Fx16ikt"
      },
      "source": [
        "**Reindicizzazione delle date, così da ottenere date ordinate senza buchi, creando una lista di dataframe col groupby delle variabili d'interesse**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDw7X2LU6ikt"
      },
      "outputs": [],
      "source": [
        "def makeGroupByCombo(dataframe_combinazioni, N_COMB = 10, N_FEAT_COMB = 4):\n",
        "    '''\n",
        "    Scelgo quante feature considerare per le combinazioni e quante combinazioni prendere \n",
        "    Per ogni combinazione aggruppa le vendite per mese e riempe con 0 in mesi senza vendite\n",
        "    '''\n",
        "    d = list()\n",
        "    for i in range(0,N_COMB):#il valore centrale dipende da quante combinazioni teniamo in considerazione: nel nostro caso 10(0->9)\n",
        "        d.append(dataframe_combinazioni[N_FEAT_COMB][i])\n",
        "\n",
        "        d[i] = d[i].groupby([d[i].index,'Divisione', 'Sub_pool', 'Cliente', \n",
        "                                'Mezzo_trasporto', 'Funzione_commerciale'])[['Vendite_consuntive','Vendite_programmate']].sum()\n",
        "        d[i] = d[i].reset_index()\n",
        "        d[i] = d[i].set_index('Data')\n",
        "        d[i] = d[i][['Divisione', 'Sub_pool', 'Cliente', 'Mezzo_trasporto', \n",
        "                     'Funzione_commerciale','Vendite_consuntive','Vendite_programmate']]\n",
        "        \n",
        "        #Se non è presente almeno una vendita per ogni mese, aggiungo vendite a 0 per i mesi mancanti\n",
        "        if d[0].shape[0] > d[i].shape[0]:     \n",
        "            # Reindicizzazione del DataFrame con i valori mancanti sostituiti da NaN\n",
        "            d[i] = d[i].reindex(new_index)\n",
        "            d[i] = d[i].reset_index()\n",
        "            d[i] = d[i].rename(columns={\"index\": \"Data\"})\n",
        "            d[i] = d[i].set_index('Data')\n",
        "            # Sostituzione di NaN con 0\n",
        "            d[i] = d[i].fillna(0)\n",
        "    return d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzeSvavK6iku"
      },
      "source": [
        "**Le date nuove sono state riempite con degli zeri partendo dall'assunzione che non ci siano state vendite nei mesi mancanti**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "tUnkZmgM6iku"
      },
      "outputs": [],
      "source": [
        "features = ['Divisione', 'Sub_pool', 'Cliente', 'Mezzo_trasporto', 'Funzione_commerciale']\n",
        "N_COMB = 10\n",
        "N_FEAT_COMB = 4\n",
        "combo = makeBestComb(df, features, N_COMB)\n",
        "df_comb = makeDataframeFromCombo(df, combo, features, N_COMB)\n",
        "display(df_comb[N_FEAT_COMB][0])\n",
        "df_comb_groupby = makeGroupByCombo(df_comb, N_COMB, N_FEAT_COMB)\n",
        "display(df_comb_groupby[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meWxfHya6iku"
      },
      "source": [
        "**Plot  delle autocorrelazioni per capire quanti e quali lag sono significativi**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jc6cn7Z-6iku"
      },
      "outputs": [],
      "source": [
        "for i in range(0,N_COMB,1):\n",
        "    \n",
        "    plot_acf(df_comb_groupby[i]['Vendite_consuntive'],title=f'Autocorrelazione dataframe {i}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzCU0w3P6iku"
      },
      "source": [
        "**Si denoti come in tutti i dataframe il primo lag è il più significativo, ogni tanto il secondo e di rado il terzo, da qui la scelta di usare 3 lag**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSC9k_D76iku"
      },
      "source": [
        "**Creazione dei dataframe laggati**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOj3tIaB6ikv"
      },
      "outputs": [],
      "source": [
        "def makeLagged(dataframe,n_mesi):\n",
        "    '''\n",
        "    Richiama funzione per lag, rimposta la data come index\n",
        "    n_mesi : numero mesi di lag\n",
        "    '''\n",
        "    dataframe_lagged = makeDataLagged(dataframe[['Vendite_consuntive']],n_mesi)\n",
        "    dataframe_lagged = dataframe_lagged.reset_index()\n",
        "    dataframe_lagged = dataframe_lagged.set_index('Data')\n",
        "    return dataframe_lagged"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tu2qx7Gp6ikv"
      },
      "source": [
        "**Creazione della suddivisione dei mesi di train e test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHUfdCih6ikv"
      },
      "outputs": [],
      "source": [
        "def makeMonths(dataframe, n_mesi = 3):\n",
        "    '''\n",
        "    Costruisce un dizionario con coppia (mese di train : mese di test)\n",
        "    Per tutti i mesi presenti nel dataframe\n",
        "    n_mesi : numero mesi di train\n",
        "    '''\n",
        "    mesi_train = list()\n",
        "    mesi_train = dataframe.index.strftime(\"%Y-%m\").tolist()\n",
        "    mesi_test = mesi_train[:]\n",
        "    for i in range(n_mesi):\n",
        "        mesi_test.pop(0)\n",
        "    mesi = dict(zip(mesi_train, mesi_test))\n",
        "    return mesi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z695JfnN6ikv"
      },
      "source": [
        "**Estrazione di altre feature dalla serie temporale, usando il rolling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDC7PnUb6ikv"
      },
      "outputs": [],
      "source": [
        "def makeRolling(dataframe_lagged, mesi_rolling = 1):\n",
        "    '''\n",
        "    Calcola il rolling dei mesi precedenti(media, mediana, minimo e massimo) e li salva nel dataframe\n",
        "    \n",
        "    '''\n",
        "    dataframe_lagged['Rolled_mean']=dataframe_lagged['var1(t)'].rolling(mesi_rolling).mean()\n",
        "    dataframe_lagged['Rolled_median']=dataframe_lagged['var1(t)'].rolling(mesi_rolling).median()\n",
        "    dataframe_lagged['Min']=dataframe_lagged['var1(t)'].rolling(mesi_rolling).min()\n",
        "    dataframe_lagged['Max']=dataframe_lagged['var1(t)'].rolling(mesi_rolling).max()\n",
        "    \n",
        "    return dataframe_lagged"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4ep9w8K6ikv"
      },
      "source": [
        "**Funzione per allenare i modelli precedentemente scelti inserendo gli input richiesti**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHSMM9g86ikv"
      },
      "outputs": [],
      "source": [
        "def makeModels(str_model, dataframe, dataframe_lagged, mesi, n_mesi_train = 3, n_mesi_lagged = 3, lag = ''):\n",
        "    dataframe_lagged = makeRolling(dataframe_lagged, mesi_rolling = 1)\n",
        "    \n",
        "    predizioni = list()\n",
        "    mae = list()\n",
        "    \n",
        "    #train e test per ogni modello inserito prima a dizionario\n",
        "    for mese_train, mese_test in mesi.items():\n",
        "        fine_mese_train = np.datetime64(mese_train) + np.timedelta64(n_mesi_train-1, 'M')\n",
        "        X_train = dataframe_lagged.loc[mese_train:fine_mese_train].drop('var1(t)', axis=1).fillna(0).values\n",
        "        X_test = dataframe_lagged.loc[mese_test].drop('var1(t)', axis=1).fillna(0).values\n",
        "        y_train = dataframe_lagged.loc[mese_train:fine_mese_train]['var1(t)'].fillna(0).values\n",
        "        y_test = dataframe_lagged.loc[mese_test]['var1(t)'].fillna(0).values\n",
        "        \n",
        "        #Randomized search per ottimizzazione degli iperparametri dei vari modelli\n",
        "        model = models[str_model]\n",
        "        param_search = param_grid[str_model]\n",
        "        neg_mae = make_scorer(mean_absolute_error, greater_is_better=False)\n",
        "        tscv = TimeSeriesSplit(n_splits=2)\n",
        "        rs = RandomizedSearchCV(estimator=model, cv=tscv, param_distributions=param_search, scoring = neg_mae)\n",
        "        rs.fit(X_train, y_train)\n",
        "        best_score = rs.best_score_\n",
        "        best_model = rs.best_estimator_\n",
        "        y_pred = best_model.predict(X_test)\n",
        "        predizioni.append(y_pred[0])\n",
        "        mae.append(mean_absolute_error(y_true=y_test, y_pred=y_pred))\n",
        "    \n",
        "    #predizione vendite e calcolo delta\n",
        "    for i in range(n_mesi_train):\n",
        "        predizioni.insert(0,0)\n",
        "        mae.insert(0,0)\n",
        "        \n",
        "    for i in range(n_mesi_lagged):\n",
        "        predizioni.insert(0,0)\n",
        "        mae.insert(0,0)\n",
        "    if lag == '':\n",
        "        dataframe['Vendite_prev_N_Mesi_'+str(n_mesi_train)] = predizioni\n",
        "        dataframe['Mae_N_Mesi_'+str(n_mesi_train)] = mae\n",
        "        dataframe['Delta_ass_N_Mesi_'+str(n_mesi_train)] = dataframe['Vendite_consuntive'] - dataframe['Vendite_prev_N_Mesi_'+str(n_mesi_train)]\n",
        "        dataframe['Delta_perc_N_Mesi_'+str(n_mesi_train)] = ((dataframe['Vendite_consuntive'] -\n",
        "        dataframe['Vendite_prev_N_Mesi_'+str(n_mesi_train)]) / dataframe['Vendite_prev_N_Mesi_'+str(n_mesi_train)]).round(3)\n",
        "    else:\n",
        "        dataframe['Vendite_prev_N_Mesi_'+lag+str(n_mesi_lagged)] = predizioni\n",
        "        dataframe['Mae_N_Mesi_'+lag+str(n_mesi_lagged)] = mae\n",
        "        dataframe['Delta_ass_N_Mesi_'+lag+str(n_mesi_lagged)] = dataframe['Vendite_consuntive'] - dataframe['Vendite_prev_N_Mesi_'+lag+str(n_mesi_lagged)]\n",
        "        dataframe['Delta_perc_N_Mesi_'+lag+str(n_mesi_lagged)] = ((dataframe['Vendite_consuntive'] -\n",
        "        dataframe['Vendite_prev_N_Mesi_'+lag+str(n_mesi_lagged)]) / dataframe['Vendite_prev_N_Mesi_'+lag+str(n_mesi_lagged)]).round(3) \n",
        "\n",
        "    return dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG_14rrM6ikw"
      },
      "source": [
        "**Calcolo della media dei forecast dei vari modelli**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSBDpyGH6ikw"
      },
      "outputs": [],
      "source": [
        "def makeMeanForecast(dataframe, n_mesi = 3, lag = ''):\n",
        "    '''\n",
        "    Funzione di calcolo della media dei forecast dei vari modelli a seconda del numero di mesi scelto, minimo 3\n",
        "    '''\n",
        "    \n",
        "    previste = dataframe.filter(regex='prev_N_Mesi_'+lag+str(n_mesi))\n",
        "    \n",
        "    previste['Final_forecast'] = previste.mean(axis=1)\n",
        "    \n",
        "    dataframe['Final_forecast_N_Mesi_'+lag+str(n_mesi)] = previste['Final_forecast']\n",
        "    \n",
        "    return dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvtOuJY26ikw"
      },
      "source": [
        "**Creazione dei dataframe con i forecast per mese e forecast finale per ogni numero di mese di train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BMRrGRn6ikx"
      },
      "outputs": [],
      "source": [
        "def makeForecast(dataframe, n_mesi_max = 3, n_mesi_def = 3, lag = ''):\n",
        "    '''\n",
        "    Funzione che crea un dataframe di vari forecast, basati sul range di mesi scelto come train (e lag) \n",
        "    '''\n",
        "    df_pred = {}\n",
        "    dataframe_model = dataframe.copy()\n",
        "    for str_model, model in models.items():\n",
        "        for n_mesi in range(3,n_mesi_max+1):\n",
        "            if lag == '':\n",
        "                dataframe_lagged = makeLagged(dataframe,n_mesi_def)\n",
        "                mesi = makeMonths(dataframe_lagged, n_mesi)\n",
        "                df_pred[str_model] = makeModels(str_model, dataframe_model, dataframe_lagged, mesi, n_mesi_train = n_mesi)\n",
        "\n",
        "            else:\n",
        "                dataframe_lagged = makeLagged(dataframe,n_mesi)\n",
        "                mesi = makeMonths(dataframe_lagged, n_mesi_def)\n",
        "                df_pred[str_model] = makeModels(str_model, dataframe_model, dataframe_lagged, mesi, n_mesi_lagged = n_mesi, lag = lag)\n",
        "        #print(str_model)\n",
        "        #display(df_pred[str_model])\n",
        "                \n",
        "    data = df_pred[str_model].reset_index()#così ho un indice temporale della lunghezza giusta            \n",
        "                  \n",
        "    df_forecast=pd.DataFrame()\n",
        "    df_forecast['Data'] = data['Data'].values\n",
        "    \n",
        "    for str_model, model in models.items():\n",
        "        for mese in range(3, n_mesi_max+1):\n",
        "            df_forecast[str_model+'_Vendite_prev_N_Mesi_'+lag+str(mese)] = df_pred[str_model]['Vendite_prev_N_Mesi_'+lag+str(mese)].values\n",
        "            df_forecast[str_model+'_Delta_ass_N_Mesi_'+lag+str(mese)] = df_pred[str_model]['Delta_ass_N_Mesi_'+lag+str(mese)].values\n",
        "            df_forecast[str_model+'_Delta_perc_N_Mesi_'+lag+str(mese)] = df_pred[str_model]['Delta_perc_N_Mesi_'+lag+str(mese)].values\n",
        "            df_forecast[str_model+'Mae_N_Mesi_'+lag+str(mese)] = df_pred[str_model]['Mae_N_Mesi_'+lag+str(mese)].values\n",
        "\n",
        "    #Ora faccio la media dei forecast e la metto nella colonna final forecast\n",
        "    df_forecast = df_forecast.set_index('Data')\n",
        "    for n_mesi in range(3,n_mesi_max+1):\n",
        "        df_forecast = makeMeanForecast(df_forecast, n_mesi, lag)\n",
        "\n",
        "    dataframe = pd.concat([dataframe,df_forecast], axis=1)\n",
        "    \n",
        "    for n_mesi in range(3,n_mesi_max+1):\n",
        "        dataframe['Delta_ass_finale_N_Mesi_'+lag+str(n_mesi)]=dataframe['Vendite_consuntive']-dataframe['Final_forecast_N_Mesi_'+lag+str(n_mesi)]\n",
        "        dataframe['Delta_perc_nuovo_N_Mesi_'+lag+str(n_mesi)]=((dataframe['Vendite_consuntive']-dataframe['Final_forecast_N_Mesi_'+lag+str(n_mesi)])\n",
        "                                          /dataframe['Final_forecast_N_Mesi_'+lag+str(n_mesi)]).round(3)\n",
        "    \n",
        "    return dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fajeF2tu6ikx"
      },
      "source": [
        "**Individuazione del miglior numero di mesi di train per ogni combinazione(dataframe) e scelta(automatica) di questi**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "j-744os86ikx"
      },
      "outputs": [],
      "source": [
        "def makeListBest(dataframe_prediction, n_mesi_max):\n",
        "    anni = ['2020','2021','2022']\n",
        "    delta_assoluto = 7\n",
        "    salta = 4\n",
        "    n_mesi_best_somma = pd.DataFrame(columns=['Mesi_Best_Somma_Ass','2020','2021','2022','Somma_Delta_Assoluto'])\n",
        "    n_mesi_best_mean = pd.DataFrame(columns=['Mesi_Best_Media_Ass','2020','2021','2022','Media_Delta_Assoluto'])\n",
        "    n_mesi_best_mean_perc = pd.DataFrame(columns=['Mesi_Best_Media_Perc','Media_Delta_Percentuale'])\n",
        "    colonne = list(range(delta_assoluto+1))\n",
        "    \n",
        "    i=0\n",
        "    somma = list()\n",
        "    mean = list()\n",
        "    mean_perc = list()\n",
        "    for dataframe in dataframe_prediction:\n",
        "        print(\"Dataframe \"+str(i))\n",
        "        \n",
        "        final = dataframe.filter(regex='Delta_ass_finale')\n",
        "        final_perc = dataframe.filter(regex='Delta_perc_nuovo')\n",
        "        final_perc.replace([np.inf, -np.inf], 0, inplace=True)\n",
        "\n",
        "        dataframe = dataframe.iloc[:,colonne]\n",
        "        dataframe = pd.concat([dataframe,final], axis = 1)\n",
        "        dataframe.rename(columns = {'Delta_ass_old' : 'Delta_ass_old_00'}, inplace = True)\n",
        "        somma.append(pd.DataFrame(columns=['Colonna','2020','2021','2022','Somma_Delta_Assoluto']))\n",
        "        mean.append(pd.DataFrame(columns=['Colonna','2020','2021','2022','Media_Delta_Assoluto']))\n",
        "        mean_perc.append(pd.DataFrame(columns=['Colonna','Media_Delta_Percentuale']))\n",
        "        #display(dataframe)\n",
        "        y = 0\n",
        "        for colonna in dataframe.columns[delta_assoluto+1:]:\n",
        "            tot_delta = list()\n",
        "            mean_delta = list()\n",
        "            tot_tot_delta = 0\n",
        "            mean_mean_delta = 0\n",
        "            for anno in range(len(anni)):\n",
        "                tot_delta.append(dataframe[colonna][anni[anno]].sum())\n",
        "                mean_delta.append(dataframe[colonna][anni[anno]].mean())\n",
        "                tot_tot_delta += tot_delta[anno]\n",
        "                mean_mean_delta += mean_delta[anno]\n",
        "            #display(dataframe[colonna][y:])\n",
        "            mean_mean_delta = dataframe[colonna][y:].mean()\n",
        "            y += 1\n",
        "            newline_somma = {'Colonna' : colonna, '2020': tot_delta[0], '2021': tot_delta[1], '2022': tot_delta[2], 'Somma_Delta_Assoluto': tot_tot_delta}\n",
        "            newline_mean = {'Colonna' : colonna, '2020': mean_delta[0], '2021': mean_delta[1], '2022': mean_delta[2], 'Media_Delta_Assoluto': mean_mean_delta}\n",
        "            somma[i] = somma[i].append(newline_somma, ignore_index=True)\n",
        "            mean[i] = mean[i].append(newline_mean, ignore_index=True)\n",
        "        x = 0\n",
        "        for colonna in final_perc.columns:\n",
        "            #print(final_perc[colonna][x:])\n",
        "            mean_percentuale = final_perc[colonna][x:].mean()\n",
        "            x += 1\n",
        "            newline_mean_perc = {'Colonna' : colonna, 'Media_Delta_Percentuale' : mean_percentuale}\n",
        "            mean_perc[i] = mean_perc[i].append(newline_mean_perc, ignore_index=True)\n",
        "        \n",
        "        somma[i].set_index('Colonna', inplace = True)\n",
        "        display(somma[i])\n",
        "        min_somma = somma[i][somma[i].Somma_Delta_Assoluto.abs() == somma[i].Somma_Delta_Assoluto.abs().min()]\n",
        "        min_somma = min_somma.reset_index()\n",
        "        display(min_somma)\n",
        "        mese = min_somma.iloc[0,0][-1]\n",
        "        \n",
        "        mean[i].set_index('Colonna', inplace = True)\n",
        "        display(mean[i])\n",
        "        min_mean = mean[i][mean[i].Media_Delta_Assoluto.abs() == mean[i].Media_Delta_Assoluto.abs().min()]\n",
        "        min_mean = min_mean.reset_index()\n",
        "        display(min_mean)\n",
        "\n",
        "        mean_perc[i].set_index('Colonna', inplace = True)\n",
        "        display(mean_perc[i])\n",
        "        min_mean_perc = mean_perc[i][mean_perc[i].Media_Delta_Percentuale.abs() == mean_perc[i].Media_Delta_Percentuale.abs().min()]\n",
        "        min_mean_perc = min_mean_perc.reset_index()\n",
        "        display(min_mean_perc)\n",
        "        \n",
        "        mese_somma = min_somma.iloc[0,0][-1]\n",
        "        if (mese_somma == '0' or mese_somma == '1' or mese_somma == '2'):\n",
        "            mese_somma = min_somma.iloc[0,0][-2:]\n",
        "        newline_somma = {'Mesi_Best_Somma_Ass' : mese_somma, '2020': min_somma.iloc[0,1], '2021': min_somma.iloc[0,2], '2022': min_somma.iloc[0,3], 'Somma_Delta_Assoluto': min_somma.iloc[0,4]}\n",
        "        n_mesi_best_somma = n_mesi_best_somma.append(newline_somma, ignore_index=True)\n",
        "        \n",
        "        mese_mean = min_mean.iloc[0,0][-1]\n",
        "        if (mese_mean == '0' or mese_mean == '1' or mese_mean == '2'):\n",
        "            mese_mean = min_mean.iloc[0,0][-2:]\n",
        "        newline_mean = {'Mesi_Best_Media_Ass' : mese_mean, '2020': min_mean.iloc[0,1], '2021': min_mean.iloc[0,2], '2022': min_mean.iloc[0,3], 'Media_Delta_Assoluto': min_mean.iloc[0,4]}\n",
        "        n_mesi_best_mean = n_mesi_best_mean.append(newline_mean, ignore_index=True)\n",
        "\n",
        "        mese_mean_perc = min_mean_perc.iloc[0,0][-1]\n",
        "        if (mese_mean_perc == '0' or mese_mean_perc == '1' or mese_mean_perc == '2'):\n",
        "            mese_mean_perc = min_mean_perc.iloc[0,0][-2:]\n",
        "        newline_mean_perc = {'Mesi_Best_Media_Perc' : mese_mean_perc, 'Media_Delta_Percentuale': min_mean_perc.iloc[0,1]}\n",
        "        n_mesi_best_mean_perc = n_mesi_best_mean_perc.append(newline_mean_perc, ignore_index=True)\n",
        "        \n",
        "        i += 1\n",
        "    display(n_mesi_best_somma)\n",
        "    mesi_best_somma = n_mesi_best_somma.Mesi_Best_Somma_Ass.to_list()\n",
        "    display(n_mesi_best_mean)\n",
        "    mesi_best_mean = n_mesi_best_mean.Mesi_Best_Media_Ass.to_list()\n",
        "    display(n_mesi_best_mean_perc)\n",
        "    mesi_best_mean_perc = n_mesi_best_mean_perc.Mesi_Best_Media_Perc.to_list()\n",
        "    return n_mesi_best_somma, n_mesi_best_mean, n_mesi_best_mean_perc, mesi_best_somma, mesi_best_mean, mesi_best_mean_perc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DVulREN6ikx"
      },
      "source": [
        "**Creazione dei plot di confronto delle vendite e predizioni per ogni combinazione**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwvHgkQk6iky"
      },
      "outputs": [],
      "source": [
        "def makePlot(df_finale, n_mesi = 3, lag = '', n_dataframe = 0, metrica = 'SUM'):    \n",
        "    '''\n",
        "    Una funzione per creare un plot specifico ad ogni dataframe che viene dato come input  \n",
        "    '''\n",
        "    \n",
        "    fig = plt.figure(figsize=(15, 5))\n",
        "    ax1 = fig.add_subplot(111)\n",
        "\n",
        "    ax1.plot(df_finale['Vendite_programmate']/1000,\n",
        "    linestyle='-',\n",
        "    linewidth=2,\n",
        "    label='Vendite programmate')\n",
        "    \n",
        "    ax1.legend(frameon=False)\n",
        "\n",
        "    ax1.plot(df_finale['Vendite_consuntive']/1000,\n",
        "         linestyle='-',\n",
        "         linewidth=2,\n",
        "         label='Vendite consuntive')\n",
        "\n",
        "    ax1.legend(frameon=False)\n",
        "\n",
        "    ax1.plot(df_finale['Final_forecast_N_Mesi_'+lag+str(n_mesi)]/1000,\n",
        "         linestyle='-',\n",
        "         color='black',\n",
        "         linewidth=2,\n",
        "         label='Vendite previste')\n",
        "    \n",
        "    ax1.legend(frameon=False)\n",
        "    \n",
        "    ax1.set_ylabel('Vendite (kT)')   \n",
        "    ax1.set_xlabel('Tempo')\n",
        "\n",
        "    x = 0\n",
        "    divisione = 0\n",
        "    while divisione == 0:\n",
        "        divisione = df_finale['Divisione'].values[x] \n",
        "        cliente = df_finale['Cliente'].values[x]\n",
        "        sub_pool= df_finale['Sub_pool'].values[x]\n",
        "        mezzo_trasporto = df_finale['Mezzo_trasporto'].values[x]\n",
        "        funzione_commerciale = df_finale['Funzione_commerciale'].values[x]\n",
        "        x += 1\n",
        "\n",
        "    divisione= divisione.capitalize()\n",
        "    cliente= cliente.capitalize()\n",
        "    sub_pool= sub_pool.capitalize()\n",
        "    mezzo_trasporto = mezzo_trasporto.capitalize()\n",
        "    funzione_commerciale = funzione_commerciale.capitalize()\n",
        "       \n",
        "    plt.title(f'2020-2022 - Combinazione:  \" {divisione} - {cliente} - {sub_pool} - {mezzo_trasporto} - {funzione_commerciale}\" \\n Numero mesi train '+lag+': '+str(n_mesi) + ' , Metrica : '+metrica, fontsize=12)\n",
        "    plt.inferno()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/drive/MyDrive/Università/Master/Grafici/GraficiPrevisioniCombinazione'+metrica+lag+str(n_dataframe))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0j6Nrms6iky"
      },
      "source": [
        "**Ottenimento del risultato finale**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "198mzSKB6iky"
      },
      "outputs": [],
      "source": [
        "i = 0\n",
        "d_pred = list()\n",
        "N_MESI_LAGGED_MAX = 3\n",
        "N_MESI_TRAIN_MAX = 8\n",
        "\n",
        "#Scelti i mesi di lag e di train, si applica la funzione makeForecast che contiene tutte le altre al \n",
        "#fine di ottenere una predizione\n",
        "#i primi 6 mesi vengono rimosssi perchè in ogni forecast almeno abbiamo 3 mesi di lag + 3 di train\n",
        "#questi ultimi(mesi di train) aumentano in base alla scelta del numero massimo scelto\n",
        "\n",
        "for dataframe in df_comb_groupby:\n",
        "    dataframe = dataframe[['Divisione','Sub_pool','Cliente','Mezzo_trasporto', 'Funzione_commerciale',\n",
        "                           'Vendite_programmate','Vendite_consuntive']]\n",
        "    dataframe['Delta_ass_old']=(dataframe['Vendite_consuntive']-dataframe['Vendite_programmate'])\n",
        "    dataframe['Delta_perc_old']=((dataframe['Vendite_consuntive']-dataframe['Vendite_programmate'])\n",
        "                                            /dataframe['Vendite_programmate']).round(3)\n",
        "    \n",
        "    dataframe = makeForecast(dataframe, n_mesi_max = N_MESI_TRAIN_MAX)       \n",
        "    dataframe = dataframe.iloc[6:,:]\n",
        "    display(dataframe)\n",
        "    d_pred.append(dataframe)\n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axb42bSh6ikz"
      },
      "outputs": [],
      "source": [
        "N_MESI_TRAIN_MAX = 8\n",
        "score_sum, score_mean, score_mean_perc, mesi_best_sum, mesi_best_mean, mesi_best_perc = makeListBest(d_pred, N_MESI_TRAIN_MAX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdgeSUJG6ikz"
      },
      "source": [
        "**Plot dei risultati (3 lag)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "N2_LTHgJ6ikz"
      },
      "outputs": [],
      "source": [
        "print(mesi_best_sum)\n",
        "mesi_best = mesi_best_sum.copy()\n",
        "x = 0\n",
        "for dataframe in d_pred:\n",
        "    mese = mesi_best.pop(0)\n",
        "    makePlot(dataframe,mese, n_dataframe=x)\n",
        "    x += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfFzLkK66ik0"
      },
      "outputs": [],
      "source": [
        "print(mesi_best_mean)\n",
        "mesi_best = mesi_best_mean.copy()\n",
        "x = 0\n",
        "for dataframe in d_pred:\n",
        "    mese = mesi_best.pop(0)\n",
        "    makePlot(dataframe,mese, n_dataframe = x, metrica = 'MEAN')\n",
        "    x += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ebu_UOu6ik0"
      },
      "outputs": [],
      "source": [
        "score_ass = pd.concat([score_sum, score_mean], axis = 1)\n",
        "display(score_ass)\n",
        "score = pd.concat([score_ass, score_mean_perc], axis = 1)\n",
        "score_redu = score[['Mesi_Best_Somma_Ass', 'Mesi_Best_Media_Ass', 'Mesi_Best_Media_Perc', 'Somma_Delta_Assoluto', 'Media_Delta_Assoluto', 'Media_Delta_Percentuale']]\n",
        "score_redu\n",
        "#dfi.export(score_redu, '/content/drive/MyDrive/Università/Master/Grafici/Score_Best.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4lm9CDO6ik0"
      },
      "source": [
        "**In questo caso vengono variati i lag e vengono tenuti costanti i mesi di train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GaeReCN6ik0"
      },
      "outputs": [],
      "source": [
        "i = 0\n",
        "d_pred_lag = list()\n",
        "N_MESI_LAGGED_MAX = 12\n",
        "N_MESI_TRAIN_MAX = 3\n",
        "\n",
        "#tale parte di codice è stata aggiunta per dare la massima libertà di personalizzazione della soluzione \n",
        "#così da poter non solo scegliere di variare i mesi di train, ma pure i lag, anche se dall'acf è stato visto che 3 sembra\n",
        "#essere il numero perfetto di lag\n",
        "\n",
        "for dataframe in df_comb_groupby:\n",
        "    dataframe = dataframe[['Divisione','Sub_pool','Cliente','Mezzo_trasporto', 'Funzione_commerciale',\n",
        "                           'Vendite_programmate','Vendite_consuntive']]\n",
        "    dataframe['Delta_ass_old']=(dataframe['Vendite_consuntive']-dataframe['Vendite_programmate'])\n",
        "    dataframe['Delta_perc_old']=((dataframe['Vendite_consuntive']-dataframe['Vendite_programmate'])\n",
        "                                            /dataframe['Vendite_programmate']).round(3)\n",
        "    \n",
        "    dataframe = makeForecast(dataframe, n_mesi_max = N_MESI_LAGGED_MAX, lag='Lag_')       \n",
        "    #print('dataframe_pred')\n",
        "    #display(dataframe_pred)\n",
        "    dataframe = dataframe.iloc[6:,:]\n",
        "    d_pred_lag.append(dataframe)\n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_muiWv16ik1"
      },
      "outputs": [],
      "source": [
        "N_MESI_LAGGED_MAX = 12\n",
        "score_sum_lag, score_mean_lag, score_mean_perc_lag, mesi_best_sum_lag, mesi_best_mean_lag, mesi_best_perc_lag = makeListBest(d_pred_lag, N_MESI_LAGGED_MAX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2FTyVCr6ik1"
      },
      "outputs": [],
      "source": [
        "print(mesi_best_sum_lag)\n",
        "mesi_best = mesi_best_sum_lag.copy()\n",
        "x = 0\n",
        "for dataframe in d_pred_lag:\n",
        "    mese = mesi_best.pop(0)\n",
        "    makePlot(dataframe, mese, lag = 'Lag_', n_dataframe=x)\n",
        "    x += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sP1zigvC6ik2"
      },
      "outputs": [],
      "source": [
        "print(mesi_best_mean_lag)\n",
        "mesi_best = mesi_best_mean_lag.copy()\n",
        "x = 0\n",
        "for dataframe in d_pred_lag:\n",
        "    mese = mesi_best.pop(0)\n",
        "    makePlot(dataframe,mese, n_dataframe = x, lag = 'Lag_', metrica = 'MEAN')\n",
        "    x += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xo5e67Pp6ik2"
      },
      "source": [
        "**Visione dei differenti punteggi al variare dei mesi di lag e di train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amAK6WfC6ik2"
      },
      "outputs": [],
      "source": [
        "score_ass_lag = pd.concat([score_sum_lag, score_mean_lag], axis = 1)\n",
        "display(score_ass_lag)\n",
        "score_lag = pd.concat([score_ass_lag, score_mean_perc_lag], axis = 1)\n",
        "score_redu_lag = score[['Mesi_Best_Somma_Ass', 'Mesi_Best_Media_Ass', 'Mesi_Best_Media_Perc', 'Somma_Delta_Assoluto', 'Media_Delta_Assoluto', 'Media_Delta_Percentuale']]\n",
        "score_redu_lag"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def makePlotDelta(df_finale, n_mesi = 3, lag = '', n_dataframe = 0, metrica = 'SUM'):    \n",
        "    '''\n",
        "    Una funzione per creare un plot specifico ad ogni dataframe che viene dato come input  \n",
        "    '''\n",
        "    \n",
        "    fig = plt.figure(figsize=(15, 5))\n",
        "    ax1 = fig.add_subplot(111)\n",
        "\n",
        "    ax1.plot(df_finale['Delta_ass_old']/1000,\n",
        "    linestyle='-',\n",
        "    linewidth=2,\n",
        "    label='Delta Assoluto Vecchio')\n",
        "    \n",
        "    ax1.legend(frameon=False)\n",
        "\n",
        "\n",
        "    ax1.plot(df_finale['Delta_ass_finale_N_Mesi_'+lag+str(n_mesi)]/1000,\n",
        "         linestyle='-',\n",
        "         #color='black',\n",
        "         linewidth=2,\n",
        "         label='Delta Assoluto Soluzione')\n",
        "    \n",
        "    ax1.legend(frameon=False)\n",
        "    \n",
        "    ax1.set_ylabel('Delta Assoluto Vendite [kT]')   \n",
        "    ax1.set_xlabel('Tempo')\n",
        "\n",
        "    x = 0\n",
        "    divisione = 0\n",
        "    while divisione == 0:\n",
        "        divisione = df_finale['Divisione'].values[x] \n",
        "        cliente = df_finale['Cliente'].values[x]\n",
        "        sub_pool= df_finale['Sub_pool'].values[x]\n",
        "        mezzo_trasporto = df_finale['Mezzo_trasporto'].values[x]\n",
        "        funzione_commerciale = df_finale['Funzione_commerciale'].values[x]\n",
        "        x += 1\n",
        "\n",
        "    divisione= divisione.capitalize()\n",
        "    cliente= cliente.capitalize()\n",
        "    sub_pool= sub_pool.capitalize()\n",
        "    mezzo_trasporto = mezzo_trasporto.capitalize()\n",
        "    funzione_commerciale = funzione_commerciale.capitalize()\n",
        "       \n",
        "    plt.title(f'2020-2022 - Combinazione:  \" {divisione} - {cliente} - {sub_pool} - {mezzo_trasporto} - {funzione_commerciale}\" ', fontsize=12)\n",
        "    plt.inferno()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/drive/MyDrive/Università/Master/Grafici/GraficiDeltaAssoluto'+metrica+lag+str(n_dataframe))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "8n3GHHkF3o-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mesi_best_mean)\n",
        "mesi_best = mesi_best_mean.copy()\n",
        "x = 0\n",
        "for dataframe in d_pred:\n",
        "    mese = mesi_best.pop(0)\n",
        "    makePlotDelta(dataframe, 3, n_dataframe = x, metrica = 'MEAN')\n",
        "    x += 1"
      ],
      "metadata": {
        "id": "WrNR4jbFucwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "krqYKaC2_e2_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "910a4752072563574700c10bb1afe8c51532c9ef994d583c96c3a980389e89e1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}